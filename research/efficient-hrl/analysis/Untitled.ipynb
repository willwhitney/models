{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willwhitney/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "alt.renderers.enable('notebook')\n",
    "\n",
    "# job_regex = \"bogo2*\"\n",
    "# job_dirs = glob.glob(\"/mnt/fair/code/neural-dynamics/results/\" + job_regex)\n",
    "basedir = '../'\n",
    "\n",
    "def load_json(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        contents = json.load(f)\n",
    "    return contents\n",
    "\n",
    "def load_multiline_json(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        contents = [json.loads(line) for line in f]\n",
    "    return contents\n",
    "\n",
    "def load_jobs(regex):\n",
    "    job_dirs = glob.glob(basedir + \"results/\" + regex)\n",
    "\n",
    "    data_dict = {}\n",
    "    for job in job_dirs:\n",
    "        job_name = os.path.basename(os.path.normpath(job))\n",
    "        options = load_json(job + '/opt.json')\n",
    "        if os.path.exists(job + '/mpc_results.json'):\n",
    "            results = load_json(job + '/mpc_results.json')\n",
    "        else:\n",
    "            continue\n",
    "        job_data = {**options, **results, 'name': job_name}\n",
    "        data_dict[job_name] = job_data\n",
    "\n",
    "    return pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "\n",
    "def load_multiline_jobs(regex):\n",
    "    job_dirs = glob.glob(basedir + \"results/\" + regex)\n",
    "\n",
    "    rows = []\n",
    "    option_keys = set()\n",
    "    for job in job_dirs:\n",
    "        job_name = os.path.basename(os.path.normpath(job))\n",
    "        options = load_json(job + '/opt.json')\n",
    "        if os.path.exists(job + '/mpc_results.json'):\n",
    "            results = load_multiline_json(job + '/mpc_results.json')\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        for result in results:\n",
    "            rows.append({**options, **result})\n",
    "        option_keys = option_keys | set(options.keys())\n",
    "\n",
    "    data_dict = {i: rows[i] for i in range(len(rows))}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "    return df, list(option_keys)\n",
    "\n",
    "def load_mpc_sweep(regex):\n",
    "    job_dirs = glob.glob(basedir + \"results/\" + regex)\n",
    "\n",
    "    rows = []\n",
    "    option_keys = set()\n",
    "    for job in job_dirs:\n",
    "        job_name = os.path.basename(os.path.normpath(job))\n",
    "        options = load_json(job + '/opt.json')\n",
    "        if os.path.exists(job + '/mpc_sweep.json'):\n",
    "            results = load_multiline_json(job + '/mpc_sweep.json')\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        for result in results:\n",
    "            rows.append({**options, **result, 'name': job_name})\n",
    "        option_keys = option_keys | set(options.keys())\n",
    "\n",
    "    data_dict = {i: rows[i] for i in range(len(rows))}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "    df['name'] = df['name'] + '_step' + df['plan_steps'].astype(str)\n",
    "    if 'plan_cands' in df:\n",
    "        df['name'] += '_cand' + df['plan_cands'].astype(str)\n",
    "    return df, list(option_keys)\n",
    "\n",
    "def load_current_data():\n",
    "    csv_data = pd.read_csv(\"Swimmer MPC results - Raw data.csv\")\n",
    "    json_cem_data = load_jobs(\"cem_optim1*\")\n",
    "    other_json_data = load_jobs(\"cem_batched*\")\n",
    "    json_bogo_data = load_jobs(\"bogo2*\")\n",
    "    data = pd.concat([csv_data, json_cem_data, json_bogo_data, other_json_data], sort=True)\n",
    "    return data\n",
    "\n",
    "def sel_hist(brush, column):\n",
    "    hist = alt.Chart().mark_bar().encode(\n",
    "            y='{}:O'.format(column),\n",
    "#             color='{}:O'.format(column),\n",
    "            x='count({}):Q'.format(column)\n",
    "        ).transform_filter(\n",
    "            brush\n",
    "        )\n",
    "    return hist\n",
    "\n",
    "def baselines():\n",
    "    baseline_data = pd.DataFrame([\n",
    "        {\"mean_R\": 110, \"name\": \"PPO\"},\n",
    "        {\"mean_R\": 30, \"name\": \"Berkeley\"},\n",
    "    ])\n",
    "\n",
    "    rule = alt.Chart(baseline_data).mark_rule().encode(y='mean_R',)\n",
    "\n",
    "    text = alt.Chart(baseline_data).mark_text(\n",
    "        align='left', dx=-330, dy=-5\n",
    "    ).encode(\n",
    "        y='mean_R',\n",
    "        text='name'\n",
    "    )\n",
    "    return rule + text\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "def load_tf(dirname, env=\"VisibleSwimmer-v2\"):\n",
    "    prefix = basedir + \"tboard/{}/\".format(env)\n",
    "    dirname = prefix + dirname\n",
    "    \n",
    "#     print(dirname + '/events.out.tfevents*')\n",
    "    matching_paths = reversed(sorted(glob.glob(dirname + '/events.out.tfevents*')))\n",
    "    all_dframes = []\n",
    "    for dirname in matching_paths:\n",
    "#         print(dirname)\n",
    "\n",
    "        ea = event_accumulator.EventAccumulator(dirname, size_guidance={event_accumulator.SCALARS: 0})\n",
    "        ea.Reload()\n",
    "        dframes = {}\n",
    "        mnames = ea.Tags()['scalars']\n",
    "\n",
    "        for n in mnames:\n",
    "            dframes[n] = pd.DataFrame(ea.Scalars(n), columns=[\"wall_time\", \"epoch\", n.replace('val/', '')])\n",
    "            dframes[n].drop(\"wall_time\", axis=1, inplace=True)\n",
    "            dframes[n] = dframes[n].set_index(\"epoch\")\n",
    "\n",
    "        dframes = [v for k,v in dframes.items()]\n",
    "        all_dframes += dframes\n",
    "\n",
    "        if len(dframes) > 0:\n",
    "            full_dframe = pd.concat(dframes, axis=1, sort=False)\n",
    "            if 'train' in full_dframe:\n",
    "                full_dframe['train_loss'] = full_dframe['train']\n",
    "            all_dframes.append(full_dframe)\n",
    "    if len(all_dframes) > 0:\n",
    "        return pd.concat(all_dframes, sort=False)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_tf_jobs(regex, env=\"VisibleSwimmer-v2\"):\n",
    "    prefix = basedir + \"results/\"\n",
    "    job_dirs = glob.glob(prefix + regex)\n",
    "\n",
    "    rows = []\n",
    "    for job in job_dirs:\n",
    "#         print(job)\n",
    "        job_name = os.path.basename(os.path.normpath(job))\n",
    "        options = load_json(job + '/opt.json')\n",
    "        results = load_tf(job.replace(prefix, ''), env=env)\n",
    "\n",
    "        if results is not None:\n",
    "            for opt in options:\n",
    "                results[opt] = options[opt]\n",
    "            rows.append(results)\n",
    "\n",
    "    for row in rows:\n",
    "        row['epoch'] = row.index\n",
    "        row.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat(rows, sort=False)\n",
    "    df['trajectories'] = np.floor(df['epoch'] / df['aggregate_every']) * df['aggregate_steps']\n",
    "    return df.reset_index()\n",
    "\n",
    "def last_epoch(paths):\n",
    "    last = -1\n",
    "    for path in paths:\n",
    "        try:\n",
    "            epoch = int(os.path.split(os.path.dirname(os.path.normpath(path)))[1])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        last = epoch if epoch > last else last\n",
    "    return last\n",
    "\n",
    "def load_tf_rollout(dirname, env=\"VisibleSwimmer-v2\", select_epoch=None):\n",
    "    prefix = basedir + \"tboard/{}/\".format(env)\n",
    "    dirname = prefix + dirname\n",
    "    \n",
    "    rollout_regex = dirname + '/*/events.out.tfevents*'\n",
    "    matching_paths = list(reversed(sorted(glob.glob(rollout_regex))))\n",
    "    all_dframes = []\n",
    "    \n",
    "    if select_epoch is None:\n",
    "        select_epoch = last_epoch(matching_paths)\n",
    "        \n",
    "    if select_epoch < 0:\n",
    "        last = last_epoch(matching_paths)\n",
    "        select_epoch = last + select_epoch + 1\n",
    "#     print(select_epoch)\n",
    "    for dirname in matching_paths:\n",
    "        try:\n",
    "            epoch = int(os.path.split(os.path.dirname(os.path.normpath(dirname)))[1])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if epoch != select_epoch:\n",
    "            continue\n",
    "#         print(epoch)\n",
    "\n",
    "        ea = event_accumulator.EventAccumulator(dirname, size_guidance={event_accumulator.SCALARS: 0})\n",
    "        ea.Reload()\n",
    "        dframes = {}\n",
    "        mnames = ea.Tags()['scalars']\n",
    "\n",
    "        for n in mnames:\n",
    "            dframes[n] = pd.DataFrame(ea.Scalars(n), columns=[\"wall_time\", \"depth\", n.replace('rollout/', '')])\n",
    "            dframes[n].drop(\"wall_time\", axis=1, inplace=True)\n",
    "            dframes[n] = dframes[n].set_index(\"depth\")\n",
    "\n",
    "        dframes = [v for k,v in dframes.items()]\n",
    "\n",
    "        if len(dframes) > 0:\n",
    "            full_dframe = pd.concat(dframes, axis=1, sort=False)\n",
    "            full_dframe['epoch'] = epoch\n",
    "            all_dframes.append(full_dframe)\n",
    "\n",
    "    if len(all_dframes) > 0:\n",
    "        return pd.concat(all_dframes, sort=False)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_tf_rollouts(regex, env=\"VisibleSwimmer-v2\", select_epoch=None):\n",
    "    prefix = basedir + \"results/\"\n",
    "    job_dirs = glob.glob(prefix + regex)\n",
    "\n",
    "    rows = []\n",
    "    for job in job_dirs:\n",
    "#         print(job)\n",
    "        job_name = os.path.basename(os.path.normpath(job))\n",
    "        options = load_json(job + '/opt.json')\n",
    "        results = load_tf_rollout(job.replace(prefix, ''), env=env, select_epoch=select_epoch)\n",
    "\n",
    "        if results is not None:\n",
    "            for opt in options:\n",
    "                results[opt] = options[opt]\n",
    "            rows.append(results)\n",
    "\n",
    "    df = pd.concat(rows, sort=False)\n",
    "    return df.reset_index()\n",
    "\n",
    "# load_tf_rollouts('rnntest', env=\"VisibleSwimmer-v2\", select_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df):\n",
    "    new_df = df.copy()\n",
    "    if 'eval_step' in df.columns:\n",
    "        steps_default = (df['eval_step'] - 1) * df['eval_freq'].astype(float)\n",
    "        new_df['steps'].fillna(steps_default, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def impute_masking(df):\n",
    "    new_df = pd.DataFrame()\n",
    "#     df['mask_x'] = df['name'].str.contains('cem3') | df['mask_x']\n",
    "#     df['mask_y'] = df['mask_y'].fillna(False)\n",
    "    for _, row in df.iterrows():\n",
    "        if not 'mask_x' in row:\n",
    "            if 'cem3' in row['name']:\n",
    "                row['mask_x'] = True\n",
    "            else:\n",
    "                row['mask_x'] = False\n",
    "        if not 'mask_y' in row:\n",
    "            row['mask_y'] = False\n",
    "        new_df = new_df.append(row)\n",
    "    return new_df\n",
    "\n",
    "def drop_short_groups(df, groups, within=None, threshold_rel=1.0, threshold_abs=0):\n",
    "    df = df.copy()\n",
    "    if within is not None:\n",
    "        within_values = df[within].unique()\n",
    "        for within_value in within_values:\n",
    "            grouped_df = df[df[within] == within_value].groupby(groups)\n",
    "            max_len = 0\n",
    "            for group in grouped_df.groups:\n",
    "                max_len = max(max_len, len(grouped_df.get_group(group).index))\n",
    "            for group in grouped_df.groups:\n",
    "                index = grouped_df.get_group(group).index\n",
    "                if len(index) < threshold_rel * max_len - threshold_abs:\n",
    "                    df = df.drop(index)\n",
    "    else:\n",
    "        grouped_df = df.groupby(groups)\n",
    "        max_len = 0\n",
    "        for group in grouped_df.groups:\n",
    "            max_len = max(max_len, len(grouped_df.get_group(group).index))\n",
    "        for group in grouped_df.groups:\n",
    "            index = grouped_df.get_group(group).index\n",
    "            if len(index) < threshold_rel * max_len - threshold_abs:\n",
    "                df = df.drop(index)\n",
    "    return df\n",
    "\n",
    "def cut(df, column, bins, groups=['name'], y_axis='mean_R', drop_short=True):\n",
    "    binned_column = 'binned_' + column\n",
    "    left_column = 'left_' + column\n",
    "    right_column = 'right_' + column\n",
    "    df = df.dropna(subset=[y_axis])\n",
    "    df[binned_column] = pd.cut(df[column], bins)\n",
    "    df[left_column] = df[binned_column].apply(lambda d: d.left)\n",
    "    df[right_column] = df[binned_column].apply(lambda d: d.right)\n",
    "    df = df.drop(binned_column, axis=1)\n",
    "    if drop_short:\n",
    "        df = drop_short_groups(df, ['name', left_column], threshold_abs=1)\n",
    "\n",
    "    df = df.groupby(list({'name', 'seed', left_column, right_column, *groups}), as_index=False).mean()\n",
    "\n",
    "    return df.dropna(subset=[y_axis])\n",
    "\n",
    "def seed_stats(df, groups=['name'], y_axis='mean_R', drop_short=True):\n",
    "    df['name'] = df['name'].str.replace('_seed[\\d]+', '')\n",
    "\n",
    "    if drop_short:\n",
    "        df = drop_short_groups(df, groups, 'name', threshold_rel=0.75)\n",
    "    for transform in ['mean', 'min', 'max', 'std', 'median']:\n",
    "        df[transform + '_' + y_axis] = df.groupby(groups)[y_axis].transform(transform)\n",
    "    \n",
    "    df['seed_count'] = df.groupby(groups)['seed'].transform('count')\n",
    "    df['seed_max'] = df.groupby('name')['seed_count'].transform(max)\n",
    "    df['seed_frac'] = df['seed_count'] / df['seed_max']\n",
    "#     df['seed_frac'] = df['seed_count'] / 8\n",
    "    df = df.drop_duplicates(subset=groups)\n",
    "    return df.dropna(subset=[y_axis])\n",
    "\n",
    "def summarize_series(df, x_axis, bins, groups=['name'], y_axis='mean_R', \n",
    "                     collect_seeds=True, drop_short_bins=True, drop_short_seeds=True):\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[y_axis])\n",
    "    groups = [group for group in groups if group != y_axis]\n",
    "    df = cut(df, x_axis, bins, groups=groups, y_axis=y_axis, drop_short=drop_short_bins)\n",
    "    if collect_seeds:\n",
    "        df = seed_stats(df, groups=['left_' + x_axis, 'name'], y_axis=y_axis, drop_short=drop_short_seeds)\n",
    "    else:\n",
    "        df['seed_count'] = 1\n",
    "        for transform in ['mean', 'min', 'max', 'median']:\n",
    "            df[transform + '_' + y_axis] = df[y_axis]\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_monitors(indir):\n",
    "    datas = []\n",
    "    infiles = glob.glob(os.path.join(indir, '*monitor.csv'))\n",
    "\n",
    "    for inf in infiles:\n",
    "#         print(inf)\n",
    "        with open(inf, 'r') as f:\n",
    "            log_line = f.readline()[1:]\n",
    "            env = json.loads(log_line)['env_id']\n",
    "            f.readline()\n",
    "            for line in f:\n",
    "                line = line.replace('\\x00', '')\n",
    "                tmp = line.split(',')\n",
    "#                 print(tmp)\n",
    "                t_time = float(tmp[2])\n",
    "                tmp = [t_time, int(tmp[1]), float(tmp[0])]\n",
    "                datas.append(tmp)\n",
    "\n",
    "    datas = sorted(datas, key=lambda d_entry: d_entry[0])\n",
    "#     print(datas)\n",
    "    result = []\n",
    "    timesteps = 0\n",
    "    for i in range(len(datas)):\n",
    "        result.append([i, timesteps, datas[i][1], datas[i][-1]])\n",
    "        timesteps += datas[i][1]\n",
    "\n",
    "#     if len(result) < bin_size:\n",
    "#         return [None, None]\n",
    "\n",
    "    result = np.array(result)\n",
    "    if len(result) == 0:\n",
    "        return None\n",
    "    trajectories, steps, duration, reward = result[:, 0], result[:, 1], result[:, 2], result[:, 3]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dict(zip(\n",
    "        ['trajectories', 'steps', 'mean_duration', 'mean_R'], \n",
    "        [trajectories, steps, duration, reward]\n",
    "    )))\n",
    "    df['trajectories'] = df.index\n",
    "    df['domain'] = env\n",
    "    return df\n",
    "\n",
    "def load_monitor_jobs(regex, basedir='../results/'):\n",
    "    job_dirs = glob.glob(os.path.join(basedir, regex))\n",
    "\n",
    "    rows = []\n",
    "    for job in job_dirs:\n",
    "        options = load_json(job + '/opt.json')\n",
    "        job_name = os.path.basename(os.path.normpath(job))\n",
    "#         print(job_name)\n",
    "        results = load_monitors(job)\n",
    "#         try:\n",
    "#             results = load_monitors(job)\n",
    "#         except:\n",
    "#             results = None\n",
    "\n",
    "        if results is not None:\n",
    "#             print(results)\n",
    "            options = {key: str(value) for (key, value) in options.items()}\n",
    "            results = results.assign(**options)\n",
    "#             for key, value in options.items():\n",
    "#                 results = results.assign(key=options[key])\n",
    "#                 print(results, key, options)\n",
    "#             rows.append({**options, **results})\n",
    "#             option_keys = option_keys | set(options.keys())\n",
    "\n",
    "\n",
    "#             rows.append(results)\n",
    "\n",
    "\n",
    "        if results is not None:\n",
    "            rows.append(results)\n",
    "    \n",
    "    df = pd.concat(rows, sort=False)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def load_evals(regex, basedir='../results/'):\n",
    "    job_dirs = glob.glob(os.path.join(basedir, regex))\n",
    "\n",
    "    rows = []\n",
    "    for job in job_dirs:\n",
    "        try:\n",
    "            options = load_json(job + '/opt.json')\n",
    "            job_name = os.path.basename(os.path.normpath(job))\n",
    "            results = np.load(os.path.join(job, 'eval.npy'))\n",
    "#             print(results)\n",
    "            if len(results.shape) == 1:\n",
    "                n = len(results)\n",
    "                results = pd.DataFrame({'eval_step': np.linspace(1, n, n), 'mean_R': results})\n",
    "            else:\n",
    "                results = pd.DataFrame({'trajectories': results[:, 0], 'steps': results[:, 1], 'mean_R': results[:, 2]})\n",
    "        except:\n",
    "#             import traceback as tb; tb.print_exc()\n",
    "            results = None\n",
    "            \n",
    "        if results is not None:\n",
    "            options = {key: str(value) for (key, value) in options.items()}\n",
    "            results = results.assign(**options)\n",
    "            rows.append(results)\n",
    "\n",
    "    df = pd.concat(rows, sort=False)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# load_evals('SPM_nostart*', basedir='../../TD3/results/')[['name', 'trajectories', 'mean_R']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = basedir + \"tboard/{}/\".format(env)\n",
    "dirname = prefix + dirname\n",
    "\n",
    "#     print(dirname + '/events.out.tfevents*')\n",
    "matching_paths = reversed(sorted(glob.glob(dirname + '/events.out.tfevents*')))\n",
    "all_dframes = []\n",
    "for dirname in matching_paths:\n",
    "#         print(dirname)\n",
    "\n",
    "    ea = event_accumulator.EventAccumulator(dirname, size_guidance={event_accumulator.SCALARS: 0})\n",
    "    ea.Reload()\n",
    "    dframes = {}\n",
    "    mnames = ea.Tags()['scalars']\n",
    "\n",
    "    for n in mnames:\n",
    "        dframes[n] = pd.DataFrame(ea.Scalars(n), columns=[\"wall_time\", \"epoch\", n.replace('val/', '')])\n",
    "        dframes[n].drop(\"wall_time\", axis=1, inplace=True)\n",
    "        dframes[n] = dframes[n].set_index(\"epoch\")\n",
    "\n",
    "    dframes = [v for k,v in dframes.items()]\n",
    "    all_dframes += dframes\n",
    "\n",
    "    if len(dframes) > 0:\n",
    "        full_dframe = pd.concat(dframes, axis=1, sort=False)\n",
    "        if 'train' in full_dframe:\n",
    "            full_dframe['train_loss'] = full_dframe['train']\n",
    "        all_dframes.append(full_dframe)\n",
    "if len(all_dframes) > 0:\n",
    "    return pd.concat(all_dframes, sort=False)\n",
    "else:\n",
    "    return None\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
